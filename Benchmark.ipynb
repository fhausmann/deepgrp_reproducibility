{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Union, Dict, Tuple\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import time\n",
    "import glob\n",
    "import subprocess\n",
    "import json\n",
    "\n",
    "from ipywidgets import IntProgress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext Cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALL_HG19 = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATADIR = './data'\n",
    "MODELDIR = './models/'\n",
    "RESULTS_PATH = './results'\n",
    "REPEATS_TO_SEARCH = [1, 2, 3, 4]\n",
    "BENCHMARKS = ['hg19/chr1.fa', 'hg38/chr1.fa', 'mm10/chr2.fa', 'hg19/chr20.fa']\n",
    "if ALL_HG19:\n",
    "    BENCHMARKS = ['hg19/chr{}.fa'.format(i) for i in range(1,23) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cython\n",
    "\n",
    "cimport cython\n",
    "cimport numpy as np\n",
    "@cython.boundscheck(False)  # Deactivate bounds checking\n",
    "@cython.wraparound(False)   # Deactivate negative indexing.\n",
    "def confusion_matrix(np.int8_t[:] true_class, np.int8_t[:] pred_class, long[:,:] output, long length):\n",
    "    \"\"\" calculate confusion matrix\"\"\"\n",
    "    cdef int i\n",
    "    for i in range(length):\n",
    "        output[true_class[i],pred_class[i]]+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_Y(filename: str, chromosom: str, length: int,\n",
    "                 repeats_to_search: List[int]) -> np.array:\n",
    "    \"\"\" Reads parse_rm file of repeats to numpy array\"\"\"\n",
    "\n",
    "    Ydata = pd.read_csv(filename, sep='\\s+', header=None, index_col=False)\n",
    "    Ydata.columns = [\n",
    "        'chromosom', 'begin', 'end', 'repeatnumber', 'repeat', 'class'\n",
    "    ]\n",
    "    Ydata = Ydata[Ydata.chromosom == chromosom]\n",
    "    Ydata.drop('chromosom', axis=1, inplace=True)\n",
    "\n",
    "    bool_series = None\n",
    "    for number in repeats_to_search:\n",
    "        if bool_series is None:\n",
    "            bool_series = (Ydata['repeatnumber'] == number)\n",
    "        else:\n",
    "            bool_series |= (Ydata['repeatnumber'] == number)\n",
    "    Ydata = Ydata[bool_series]\n",
    "    Y = np.zeros((len(repeats_to_search) + 1, length), dtype=np.int8)\n",
    "\n",
    "    def assign_toY(row):\n",
    "        Y[row['repeatnumber'], row.begin:row.end] = 1\n",
    "\n",
    "    Ydata.apply(assign_toY, axis=1)\n",
    "    del Ydata\n",
    "    return Y.argmax(axis=0).astype(np.int8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_to_array(filename: str,\n",
    "                  shape: np.array,\n",
    "                  dnabrnn: bool = False) -> np.array:\n",
    "    \"\"\"Reads dna-brnn or deepgrp file to array\"\"\"\n",
    "    headernames = [\"file\", \"chr\", \"start\", \"end\", \"class\"]\n",
    "    if dnabrnn:\n",
    "        headernames.pop(0)\n",
    "    tmp = pd.read_csv(filename, header=None, sep=\"\\t\",\n",
    "                      names=headernames).filter(headernames[-3:], axis=1)\n",
    "    Y = np.zeros(shape, dtype=np.int8)\n",
    "\n",
    "    def assign_toY(row):\n",
    "        Y[row.start:row.end] = row['class']\n",
    "\n",
    "    tmp.apply(assign_toY, axis=1)\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mcc(C):\n",
    "    \"\"\" MCC implementation based on sklearn\"\"\"\n",
    "    t_sum = C.sum(axis=1, dtype=np.float64)\n",
    "    p_sum = C.sum(axis=0, dtype=np.float64)\n",
    "    n_correct = np.trace(C, dtype=np.float64)\n",
    "    n_samples = p_sum.sum()\n",
    "    cov_ytyp = n_correct * n_samples - np.dot(t_sum, p_sum)\n",
    "    cov_ypyp = n_samples**2 - np.dot(p_sum, p_sum)\n",
    "    cov_ytyt = n_samples**2 - np.dot(t_sum, t_sum)\n",
    "    return cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(predictions_class: np.array, true_class: np.array):\n",
    "    \"\"\"Calculated important metrics.\"\"\"\n",
    "    nof_labels = len(REPEATS_TO_SEARCH) + 1\n",
    "    cnf_matrix = np.zeros((nof_labels, nof_labels), dtype=int)\n",
    "    confusion_matrix(true_class, predictions_class, cnf_matrix,\n",
    "                     true_class.shape[0])\n",
    "    true_positive = np.diag(cnf_matrix).astype(float)\n",
    "    false_positive = (cnf_matrix.sum(axis=0) - true_positive).astype(float)\n",
    "    false_negative = (cnf_matrix.sum(axis=1) - true_positive).astype(float)\n",
    "    true_negative = (\n",
    "        cnf_matrix.sum() -\n",
    "        (false_positive + false_negative + true_positive)).astype(float)\n",
    "    metrics = {}\n",
    "    # Sensitivity, hit rate, recall, or true positive rate\n",
    "    metrics[\"TPR\"] = true_positive / (true_positive + false_negative)\n",
    "    # Specificity or true negative rate\n",
    "    metrics[\"TNR\"] = true_negative / (true_negative + false_positive)\n",
    "    # Precision or positive predictive value\n",
    "    metrics[\"PPV\"] = true_positive / (true_positive + false_positive)\n",
    "    # Negative predictive value\n",
    "    metrics[\"NPV\"] = true_negative / (true_negative + false_negative)\n",
    "    # Fall out or false positive rate\n",
    "    metrics[\"FPR\"] = false_positive / (false_positive + true_negative)\n",
    "    # False negative rate\n",
    "    metrics[\"FNR\"] = false_negative / (true_positive + false_negative)\n",
    "    # False discovery rate\n",
    "    metrics[\"FDR\"] = false_positive / (true_positive + false_positive)\n",
    "    # Accuracy\n",
    "    metrics[\"ACC\"] = (true_positive + true_negative) / \\\n",
    "        (true_positive + false_positive + false_negative + true_negative)\n",
    "    # F1 -Score\n",
    "    metrics[\"F1\"] = 2 * metrics[\"TPR\"] * \\\n",
    "        metrics[\"PPV\"] / (metrics[\"TPR\"] + metrics[\"PPV\"])\n",
    "    metrics[\"TotalACC\"] = (\n",
    "        true_class == predictions_class).sum() / true_class.shape[0]\n",
    "    metrics['MCC'] = ((true_positive * true_negative) -\n",
    "                      (false_positive * false_negative)) / np.sqrt(\n",
    "                          (true_positive + false_positive) *\n",
    "                          (true_positive + false_negative) *\n",
    "                          (true_negative + false_positive) *\n",
    "                          (true_negative + false_negative))\n",
    "    metrics['totalMCC'] = mcc(cnf_matrix)\n",
    "    for key in metrics:\n",
    "        if isinstance(metrics[key], np.ndarray):\n",
    "            metrics[key] = metrics[key].tolist()\n",
    "    metrics['confusionmatrix'] = cnf_matrix.tolist()\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_all(results, modelfiles, is_dnabrnn=False):\n",
    "    \"\"\" Calculates metrics based on deepgrp or dna-brnn output file\"\"\"\n",
    "    for k in results:\n",
    "        seqlen = np.load(os.path.join(DATADIR, k + '.gz.npz'))['fwd'].shape[1]\n",
    "        foldername, chrfile = os.path.split(k)\n",
    "        filename = os.path.join(DATADIR, foldername) + \".bed\"\n",
    "        chromosom = chrfile.replace('.fa', '')\n",
    "        Ytrue = preprocess_Y(filename, chromosom, seqlen, REPEATS_TO_SEARCH)\n",
    "        for model in modelfiles:\n",
    "            modelname = model.replace(MODELDIR, \"\")\n",
    "            predfilename = \"{}_{}.fa_{}.tsv\".format(foldername, chromosom,\n",
    "                                                    modelname)\n",
    "            Ypred = file_to_array(predfilename, Ytrue.shape, is_dnabrnn)\n",
    "            metrics = calculate_metrics(Ypred, Ytrue)\n",
    "            results[k][modelname].update(metrics)\n",
    "            del Ypred\n",
    "        del Ytrue\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark(benchmarks: List[str], modelfiles: List[str], errorfile: str,\n",
    "              command: List[str]) -> Dict[str, Dict[str, float]]:\n",
    "    \"\"\"Benchmark a program\"\"\"\n",
    "    n_runs = len(benchmarks) * len(modelfiles)\n",
    "    results = dict()\n",
    "    f = IntProgress(min=0, max=n_runs)\n",
    "    display(f)\n",
    "\n",
    "    for k in benchmarks:\n",
    "        results[k] = {}\n",
    "        infile = os.path.join(DATADIR, k)\n",
    "        for model_path in modelfiles:\n",
    "            modelname = model_path.replace(MODELDIR, '')\n",
    "            print(k, modelname, end='\\t')\n",
    "            outfile = '{}_{}.tsv'.format(k, modelname).replace('/', '_')\n",
    "            with open(outfile, 'wb') as file:\n",
    "                env = os.environ.copy()\n",
    "                env[\"TF_XLA_FLAGS\"]=\"--tf_xla_auto_jit=2\"\n",
    "                #env[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
    "                start_time = time.time()\n",
    "                process = subprocess.Popen(command + [model_path, infile],\n",
    "                                           stdout=file,\n",
    "                                           stderr=subprocess.PIPE,\n",
    "                                           env=env)\n",
    "                _, errdata = process.communicate()\n",
    "                end_time = time.time()\n",
    "            runtime = end_time - start_time\n",
    "            results[k][modelname] = {'runtime': runtime}\n",
    "            with open(errorfile, 'ab') as file:\n",
    "                file.write(errdata)\n",
    "            f.value += 1\n",
    "            print(runtime)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Run DeepGRP benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "deepgrpmodels = glob.glob(os.path.join(MODELDIR, 'model_*.h5'))\n",
    "deepgrp_command = ['python3', '-m', 'deepgrp', '-t 10', \"--xla\", \"-b 4096\"]\n",
    "deepgrp_results = benchmark(BENCHMARKS, deepgrpmodels, 'deepgrp.log',\n",
    "                            deepgrp_command)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1) Calculate metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "deepgrp_results = calculate_all(deepgrp_results, deepgrpmodels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'deepgrp_gpu_results.json'\n",
    "if ALL_HG19:\n",
    "    filename = 'deepgrp_results_hg19complete.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(RESULTS_PATH, filename), 'w') as file:\n",
    "    json.dump(deepgrp_results, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Run dna-brnn benchmark "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dnabrnnmodels = glob.glob(os.path.join(MODELDIR, 'dnabrnn_model*.knm'))\n",
    "dnabrnn_command = ['dna-nn/dna-brnn', '-t 10', '-O292', '-Ai']\n",
    "dnabrnnresults = benchmark(BENCHMARKS, dnabrnnmodels, 'dnabrnn.log',\n",
    "                           dnabrnn_command)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1) Calculate metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dnabrnnresults = calculate_all(dnabrnnresults, dnabrnnmodels, is_dnabrnn=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'dnabrnn_results.json'\n",
    "if ALL_HG19:\n",
    "    filename = 'dnabrnn_results_hg19complete.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(RESULTS_PATH, filename), 'w') as file:\n",
    "    json.dump(dnabrnnresults, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:hydrogen"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
